# mtp_glm5 Test Results

**Date:** 2026-02-21 09:08:46 UTC

## Configuration

| Parameter | Value |
|-----------|-------|
| Model | GLM-5 (4-bit quantized) |
| MTP Model | GLM-5-mtp (original HF weights) |
| Mode | Single Node (hwstudio1) |
| Speculative Method | MTP (k=1) |
| KV Cache Quantization | FP8 |
| Note | MTP does NOT support TP mode — silently falls back to normal decoding |

## Single Request

| # | Status | Prompt Tokens | Completion Tokens | TTFT (ms) | Prefill tok/s | Decode tok/s | Total Time (s) |
|---|--------|---------------|-------------------|-----------|---------------|--------------|----------------|
| 1 | PASS | 17 | 8 | 3197.8 | 5.3 | 0.8 | 12.95 |

## Sequential Requests

| # | Status | Prompt Tokens | Completion Tokens | TTFT (ms) | Prefill tok/s | Decode tok/s | Total Time (s) |
|---|--------|---------------|-------------------|-----------|---------------|--------------|----------------|
| 1 | PASS | 23 | 256 | 719.1 | 32.0 | 13.3 | 20.01 |
| 2 | PASS | 24 | 256 | 764.4 | 31.4 | 16.1 | 16.70 |
| 3 | PASS | 25 | 85 | 700.0 | 35.7 | 11.0 | 8.44 |

## Summary

- **Working:** YES
- **Total requests:** 4
- **Successful:** 4
- **Failed:** 0
- **Avg TTFT:** 1345.3 ms
- **Avg prefill tok/s:** 26.1
- **Avg decode tok/s:** 10.3
- **Total output tokens:** 605
- **Avg per-request throughput:** 10.4 tok/s

## MTP Speculative Decode Stats

```json
{
  "enabled": true,
  "method": "mtp",
  "k": 1,
  "total_drafts": 6,
  "total_draft_tokens": 6,
  "total_accepted": 0,
  "acceptance_rate": 0.0
}
```

## Output Samples

### Single Request
```
The capital of France is Paris.
```

### Sequential Requests
```
The core difference between TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) comes down to how they handle the reliability and order of data transmission.

Think of it like the difference between sending a **certified mail package with tracking** (TCP) and sending a **postcard** (UDP).

Here is a detailed breakdown of the differences and when to use each.

---

### 1. TCP (Transmission Control Protocol)
**The "Reliable" Choice**

TCP is a connection-oriented protocol. It prio...
```

### Sequential Requests
```
Here is a Python function that accomplishes this. It uses a regular expression to remove all non-alphanumeric characters and then compares the string to its reverse.

```python
import re

def is_valid_palindrome(text):
    """
    Checks if a string is a valid palindrome, ignoring spaces, punctuation, and capitalization.
    """
    if not isinstance(text, str):
        return False
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove all non-alphanumeric characters (spaces,...
```

### Sequential Requests
```
Unit 734 paused its relentless sorting of scrap metal when a faint, rhythmic vibration hummed against its metallic fingertips. Tracing the sound to a crushed antique radio, it was suddenly flooded with a melody that defied its binary logic, creating a strange, pleasant error in its core processor. For the first time, the machine chose to ignore its efficiency protocols, standing perfectly still simply to let the song finish.
```


## MTP + TP Limitation

MTP (Multi-Token Prediction) does **not** support Tensor Parallel mode. When `--speculative-method mtp` is used with TP, the server silently falls back to normal (non-speculative) decoding. There is no error or warning — the only observable difference is the lack of speedup from speculative decoding.

For this reason, this test runs on **hwstudio1 only** (single node).


## Server Logs (excerpt)

### hwstudio1

*(showing last 200 of 769 lines)*

```
2026-02-21 18:08:06,072 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,135 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,197 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,260 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,323 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,386 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,449 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,512 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,512 INFO vllm_mlx.spec_decode.runtime: Spec decode auto-disable: running probe round to re-evaluate (recent acceptance rate=0.000, threshold=0.400)
2026-02-21 18:08:06,575 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,638 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,701 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,764 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,826 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,889 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,952 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:06,954 INFO vllm_mlx.scheduler: [Metal memory] active=424.4GB peak=451.5GB cache=0.0GB step=512 running=1 waiting=0
2026-02-21 18:08:07,017 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:07,079 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:07,143 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:07,205 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:07,268 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:07,331 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:07,395 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:07,458 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:07,458 DEBUG vllm_mlx.scheduler: Request 05681b2a-decc-4999-a7d3-3b558599b18c finished: length, 256 tokens
2026-02-21 18:08:07,462 INFO vllm_mlx.engine_core: [stream_outputs] 05681b2a-dec finished normally, 171 tokens in 16.3s
2026-02-21 18:08:07,462 INFO vllm_mlx.engine_core: [stream_outputs] 05681b2a-dec cleanup done
2026-02-21 18:08:07,462 INFO vllm_mlx.server: Chat completion (stream): 256 tokens in 16.28s (15.7 tok/s)
2026-02-21 18:08:07,462 INFO vllm_mlx.server: [disconnect_guard] generator exhausted normally, 174 chunks, elapsed=16.3s
2026-02-21 18:08:07,462 INFO vllm_mlx.server: [disconnect_guard] CLEANUP done, 174 chunks total, elapsed=16.3s
INFO:     100.95.28.121:55843 - "GET /v1/models HTTP/1.1" 200 OK
2026-02-21 18:08:08,017 INFO vllm_mlx.server: [REQUEST] POST /v1/chat/completions stream=True model='default' max_tokens=128 temp=0.0 msgs=1 roles=['user'] total_chars=54 tools=0 response_format=None
2026-02-21 18:08:08,017 INFO vllm_mlx.server: [REQUEST] last user message preview: 'What is the capital of France? Answer in one sentence.'
INFO:     100.95.28.121:55846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-21 18:08:08,018 INFO vllm_mlx.server: [disconnect_guard] START poll_interval=0.5s
2026-02-21 18:08:08,019 INFO vllm_mlx.server: [disconnect_guard] first chunk arrived, elapsed=0.0s
2026-02-21 18:08:08,020 INFO vllm_mlx.scheduler: [cache_fetch] request=e78d2700-bd1 HIT prompt_tokens=17 cached=17 remaining=0 time=0.000s
2026-02-21 18:08:08,020 DEBUG vllm_mlx.scheduler: Added request e78d2700-bd19-4508-959d-de036a5430d7 with 17 prompt tokens
2026-02-21 18:08:08,020 INFO vllm_mlx.engine_core: [stream_outputs] e78d2700-bd1 START waiting for tokens
2026-02-21 18:08:08,021 INFO vllm_mlx.scheduler: [schedule] request=e78d2700-bd1 uid=3 prompt_tokens=17 tokens_to_prefill=1, 17 cached max_tokens=128 running=1 waiting=0
2026-02-21 18:08:08,145 INFO vllm_mlx.scheduler: [prompt_cache_save] request=e78d2700-bd1 prompt_tokens=17 store_time=0.000s
2026-02-21 18:08:08,204 INFO vllm_mlx.engine_core: [stream_outputs] e78d2700-bd1 first token after 0.2s
2026-02-21 18:08:08,266 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:08,328 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:08,388 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:08,449 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:08,510 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:08,571 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:08,631 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:08,631 DEBUG vllm_mlx.scheduler: Request e78d2700-bd19-4508-959d-de036a5430d7 finished: stop, 8 tokens
2026-02-21 18:08:08,635 INFO vllm_mlx.engine_core: [stream_outputs] e78d2700-bd1 finished normally, 6 tokens in 0.6s
2026-02-21 18:08:08,635 INFO vllm_mlx.engine_core: [stream_outputs] e78d2700-bd1 cleanup done
2026-02-21 18:08:08,635 INFO vllm_mlx.server: Chat completion (stream): 8 tokens in 0.62s (13.0 tok/s)
2026-02-21 18:08:08,636 INFO vllm_mlx.server: [disconnect_guard] generator exhausted normally, 9 chunks, elapsed=0.6s
2026-02-21 18:08:08,636 INFO vllm_mlx.server: [disconnect_guard] CLEANUP done, 9 chunks total, elapsed=0.6s
INFO:     100.95.28.121:55847 - "GET /v1/status HTTP/1.1" 200 OK
2026-02-21 18:08:08,876 INFO vllm_mlx.server: [REQUEST] POST /v1/chat/completions stream=True model='default' max_tokens=256 temp=0.0 msgs=1 roles=['user'] total_chars=87 tools=0 response_format=None
2026-02-21 18:08:08,877 INFO vllm_mlx.server: [REQUEST] last user message preview: 'Write a short story (3-4 sentences) about a robot discovering music for the first time.'
INFO:     100.95.28.121:55848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-21 18:08:08,878 INFO vllm_mlx.server: [disconnect_guard] START poll_interval=0.5s
2026-02-21 18:08:08,878 INFO vllm_mlx.server: [disconnect_guard] first chunk arrived, elapsed=0.0s
2026-02-21 18:08:08,879 DEBUG vllm_mlx.memory_cache: [cache_fetch] LCP scan: cached_len=17 req_len=25 lcp=3
2026-02-21 18:08:08,879 DEBUG vllm_mlx.memory_cache: [cache_fetch] LCP scan: cached_len=24 req_len=25 lcp=5
2026-02-21 18:08:08,879 DEBUG vllm_mlx.memory_cache: [cache_fetch] LCP candidate: lcp=5 entry_len=24 excess=19 non_trimmable=True cache_layers=78 layer_types=['CacheList', 'CacheList', 'CacheList']
2026-02-21 18:08:08,879 INFO vllm_mlx.scheduler: [cache_fetch] request=3ddb8541-339 MISS prompt_tokens=25 time=0.000s entries=3
2026-02-21 18:08:08,880 DEBUG vllm_mlx.scheduler: Added request 3ddb8541-3390-4aef-a8e3-3ccc53e63293 with 25 prompt tokens
2026-02-21 18:08:08,880 INFO vllm_mlx.engine_core: [stream_outputs] 3ddb8541-339 START waiting for tokens
2026-02-21 18:08:08,881 INFO vllm_mlx.scheduler: [schedule] request=3ddb8541-339 uid=4 prompt_tokens=25 tokens_to_prefill=25 max_tokens=256 running=1 waiting=0
2026-02-21 18:08:09,060 INFO vllm_mlx.server: [REQUEST] POST /v1/chat/completions stream=True model='default' max_tokens=256 temp=0.0 msgs=1 roles=['user'] total_chars=90 tools=0 response_format=None
2026-02-21 18:08:09,060 INFO vllm_mlx.server: [REQUEST] last user message preview: 'Explain the difference between TCP and UDP protocols. Include when you would use each one.'
INFO:     100.95.28.121:55849 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-21 18:08:09,061 INFO vllm_mlx.server: [disconnect_guard] START poll_interval=0.5s
2026-02-21 18:08:09,061 INFO vllm_mlx.server: [disconnect_guard] first chunk arrived, elapsed=0.0s
2026-02-21 18:08:09,062 INFO vllm_mlx.scheduler: [cache_fetch] request=116de781-14f HIT prompt_tokens=23 cached=23 remaining=0 time=0.000s
2026-02-21 18:08:09,062 DEBUG vllm_mlx.scheduler: Added request 116de781-14fc-4666-916d-2552c7bed0b1 with 23 prompt tokens
2026-02-21 18:08:09,062 INFO vllm_mlx.engine_core: [stream_outputs] 116de781-14f START waiting for tokens
2026-02-21 18:08:09,276 INFO vllm_mlx.scheduler: [prefill] request=3ddb8541-339 tokens=24/25
2026-02-21 18:08:09,385 DEBUG vllm_mlx.memory_cache: Stored cache: 25 tokens, 2.62MB, total=9.3MB
2026-02-21 18:08:09,385 INFO vllm_mlx.scheduler: [prompt_cache_save] request=3ddb8541-339 prompt_tokens=25 store_time=0.000s
2026-02-21 18:08:09,443 INFO vllm_mlx.engine_core: [stream_outputs] 3ddb8541-339 first token after 0.6s
2026-02-21 18:08:09,443 INFO vllm_mlx.scheduler: [schedule] request=116de781-14f uid=5 prompt_tokens=23 tokens_to_prefill=1, 23 cached max_tokens=256 running=2 waiting=0
2026-02-21 18:08:09,544 INFO vllm_mlx.scheduler: [prompt_cache_save] request=116de781-14f prompt_tokens=23 store_time=0.000s
2026-02-21 18:08:09,704 INFO vllm_mlx.engine_core: [stream_outputs] 116de781-14f first token after 0.6s
2026-02-21 18:08:09,801 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:09,886 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:09,970 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:09,971 INFO vllm_mlx.spec_decode.runtime: Spec decode auto-disable: running probe round to re-evaluate (recent acceptance rate=0.000, threshold=0.400)
2026-02-21 18:08:10,054 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,139 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,223 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,308 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,393 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,477 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,569 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,652 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,737 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,822 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,907 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:10,996 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,080 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,164 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,248 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,336 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,420 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,505 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,589 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,674 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,758 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,842 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:11,926 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,011 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,095 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,096 INFO vllm_mlx.spec_decode.runtime: Spec decode auto-disable: running probe round to re-evaluate (recent acceptance rate=0.000, threshold=0.400)
2026-02-21 18:08:12,178 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,262 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,349 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,434 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,538 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,621 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,705 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,790 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,874 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:12,958 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,040 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,124 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,209 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,294 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,382 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,467 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,552 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,635 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,720 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,803 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,887 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:13,972 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,060 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,144 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,230 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,231 INFO vllm_mlx.spec_decode.runtime: Spec decode auto-disable: running probe round to re-evaluate (recent acceptance rate=0.000, threshold=0.400)
2026-02-21 18:08:14,315 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,398 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,483 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,567 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,651 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,735 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,819 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,903 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:14,986 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,069 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,154 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,237 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,321 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,405 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,489 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,500 INFO vllm_mlx.server: [disconnect_guard] poll #10 disconnected=False elapsed=6.6s
2026-02-21 18:08:15,584 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,668 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,757 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,762 INFO vllm_mlx.server: [disconnect_guard] poll #10 disconnected=False elapsed=6.7s
2026-02-21 18:08:15,845 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:15,930 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,014 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,098 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,182 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,268 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,352 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,354 INFO vllm_mlx.spec_decode.runtime: Spec decode auto-disable: running probe round to re-evaluate (recent acceptance rate=0.000, threshold=0.400)
2026-02-21 18:08:16,438 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,523 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,607 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,691 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,776 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,777 DEBUG vllm_mlx.scheduler: Request 3ddb8541-3390-4aef-a8e3-3ccc53e63293 finished: stop, 85 tokens
2026-02-21 18:08:16,926 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:16,988 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,049 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,050 INFO vllm_mlx.engine_core: [stream_outputs] 3ddb8541-339 finished normally, 58 tokens in 8.2s
2026-02-21 18:08:17,050 INFO vllm_mlx.engine_core: [stream_outputs] 3ddb8541-339 cleanup done
2026-02-21 18:08:17,050 INFO vllm_mlx.server: Chat completion (stream): 85 tokens in 8.17s (10.4 tok/s)
2026-02-21 18:08:17,110 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,172 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,233 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,296 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,296 INFO vllm_mlx.server: [disconnect_guard] generator exhausted normally, 61 chunks, elapsed=8.4s
2026-02-21 18:08:17,296 INFO vllm_mlx.server: [disconnect_guard] CLEANUP done, 61 chunks total, elapsed=8.4s
2026-02-21 18:08:17,358 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,419 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,480 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,542 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,603 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,664 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,727 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,788 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,849 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,911 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:17,971 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:18,033 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:18,094 INFO vllm_mlx.scheduler: [SpecDecode] steps=50, alpha=0.000, mean_accepted=0.00/1, per_pos=['0.00']
2026-02-21 18:08:18,095 INFO vllm_mlx.spec_decode.runtime: Spec decode auto-disable: running probe round to re-evaluate (recent acceptance rate=0.000, threshold=0.400)
```

