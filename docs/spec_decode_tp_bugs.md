# Speculative Decoding + Tensor Parallel ë²„ê·¸ ë¦¬í¬íŠ¸

> ì‘ì„±ì¼: 2026-02-15
> ìµœì¢… ì—…ë°ì´íŠ¸: 2026-02-16
> í™˜ê²½: 2x Mac Studio M4 Ultra 512GB, TB5 RDMA, Kimi K2.5 612GB MoE
> ì½”ë“œ: vllm-mlx develop ë¸Œëœì¹˜
> ìƒíƒœ: **ë²„ê·¸ 7 (kâ‰¥2 ë°ë“œë½) í™œë°œíˆ ì¡°ì‚¬ ì¤‘ â€” ë²„ê·¸ 6 (TP ì¶œë ¥ í’ˆì§ˆ) ë¯¸í•´ê²°**

---

## ìš”ì•½

n-gram speculative decodingì„ ë¶„ì‚° Tensor Parallel (TP=2) í™˜ê²½ì—ì„œ ë°œê²¬ëœ ë²„ê·¸ í˜„í™©:

1. **Decode hang** â€” âœ… ìˆ˜ì • ì™„ë£Œ (`0428e89`)
2. **ì¶œë ¥ corruption** â€” âš ï¸ ë¶€ë¶„ ìˆ˜ì • (`0428e89`) â€” emission ìˆ˜ì •ë¨, state accounting desync ì”ì¡´
3. **Trim edge case / batch state sync** â€” âœ… ìˆ˜ì • ì™„ë£Œ (`0428e89`, `ad2d1dc`)
4. **Memory pressure ë¬´í•œ ë£¨í”„** â€” âœ… ìˆ˜ì • ì™„ë£Œ (`0428e89`)
5. **TP ìƒ˜í”Œë§ ë™ê¸°í™”** â€” âœ… ìˆ˜ì • ì™„ë£Œ (`d11cd16`)
6. **TP ì¶œë ¥ í’ˆì§ˆ ì €í•˜** â€” â“ ë¯¸í•´ê²°
7. **kâ‰¥2 spec decode ë°ë“œë½** â€” ğŸ”´ **í™œë°œíˆ ì¡°ì‚¬ ì¤‘** (`878fc00`ì—ì„œ ì²« ì‹œë„ ì‹¤íŒ¨)

---

## ë²„ê·¸ 1: Spec Decode + TP Decode Hang â€” âœ… FIXED

**ì»¤ë°‹**: `0428e89`

**ë¬¸ì œ**: Workerì˜ `_worker_spec_decode_step()`ì—ì„œ `active_batch is None`ì¼ ë•Œ forwardë¥¼ skipí•˜ì—¬ Rank 0ì˜ all_sumê³¼ deadlock ë°œìƒ.

**ìˆ˜ì •**: `active_batch is None` ì‹œ `RuntimeError` raiseë¡œ fail-fast ì „í™˜. ì¶”ê°€ë¡œ `5f1f066`ì—ì„œ keepalive StepPlan broadcastë¡œ idle desync ë°©ì§€.

**ê²€ì¦ ìœ„ì¹˜**: `distributed_launcher.py` lines 379-385

---

## ë²„ê·¸ 2: ì¶œë ¥ Corruption (ë‹¨ì–´ ë°˜ë³µ) â€” âš ï¸ PARTIALLY FIXED â€” emission ìˆ˜ì •ë¨, state accounting desync ì”ì¡´

**ì»¤ë°‹**: `0428e89`

**ë¬¸ì œ**: Spec decode pathì—ì„œ `result.accepted_tokens`ë§Œ emití•˜ê³  `batch.y`(old y)ë¥¼ ëˆ„ë½ â†’ í† í° ì¤‘ë³µ/ëˆ„ë½.

**ìˆ˜ì •**: `committed_tokens = [batch_y] + accepted_tokens[:-1]`ë¡œ emission ë¡œì§ ìˆ˜ì •. Bonus tokenì€ ë‹¤ìŒ stepì˜ `batch.y`ë¡œ ì„¤ì •.

**ê²€ì¦ ìœ„ì¹˜**: `scheduler.py` lines 978-1003

---

## ë²„ê·¸ 3: Trim Edge Case / Worker Batch State Sync â€” âœ… FIXED

**ì»¤ë°‹**: `0428e89`, `ad2d1dc`

**ë¬¸ì œ (ì›ë˜ ë³´ê³ )**: Stop/length clipping ì‹œ bonus token over-trim ìš°ë ¤.

**ë¬¸ì œ (ì‹¤ì œ ê·¼ë³¸ ì›ì¸)**: TP workerê°€ spec decode ê²°ê³¼ ìˆ˜ì‹  í›„ `batch.tokens`ì™€ `batch.num_tokens`ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šì•„ rank ê°„ batch state desync ë°œìƒ ê°€ëŠ¥.

**ìˆ˜ì •**:
- Workerì—ì„œ `accepted_tokens`ë¡œë¶€í„° `batch.tokens`ì™€ `batch.num_tokens` ë™ê¸°í™” ë¡œì§ ì¶”ê°€ (`distributed_launcher.py` lines 424-450)
- `mx.minimum()` boundary ë³´í˜¸ë¡œ over-trim ë°©ì§€
- `mx.eval()` materializeë¡œ lazy graph ëˆ„ì  ë°©ì§€

**ê²€ì¦ ìœ„ì¹˜**: `distributed_launcher.py` lines 420-464

---

## ë²„ê·¸ 4: Memory Pressure Threshold â€” âœ… FIXED

**ì»¤ë°‹**: `0428e89`

**ë¬¸ì œ**: `engine_core.py`ì˜ thresholdê°€ 200GiB(215GB)ë¡œ í•˜ë“œì½”ë”© â†’ Kimi K2.5 (332GB) í™˜ê²½ì—ì„œ í•­ìƒ ì´ˆê³¼ â†’ ë§¤ 64 step `mx.clear_cache()` í˜¸ì¶œ.

**ìˆ˜ì •**: Thresholdë¥¼ 500GiBë¡œ ìƒí–¥. 512GB ì‹œìŠ¤í…œì˜ ~97%.

**ê²€ì¦ ìœ„ì¹˜**: `engine_core.py` line 186

---

## ì´ì „ ì„¸ì…˜ì—ì„œ ìˆ˜ì •í•œ ì½”ë“œ

### 1. cache_utils.py â€” `_trim_layer()` ì¶”ê°€
```python
def _trim_layer(layer: Any, trim_amounts: mx.array) -> None:
    """Trim a single cache layer, recursing into CacheList."""
    try:
        from mlx_lm.models.cache import CacheList
        if isinstance(layer, CacheList):
            for sub in layer.caches:
                _trim_layer(sub, trim_amounts)
            return
    except ImportError:
        caches = getattr(layer, "caches", None)
        if isinstance(caches, (tuple, list)):
            for sub in caches:
                _trim_layer(sub, trim_amounts)
            return
    layer.trim_per_sequence(trim_amounts)
```
- `batch.cache`ëŠ” `list[BatchKVCache]` (ë ˆì´ì–´ë‹¹ í•˜ë‚˜) â†’ ì´ì „ ì½”ë“œê°€ ì§ì ‘ `trim_per_sequence` í˜¸ì¶œ ì‹œ CacheList ë ˆì´ì–´ì—ì„œ í¬ë˜ì‹œ
- `batch_variable_trim()`ì´ `_trim_layer()`ë¥¼ í†µí•´ ì¬ê·€ì ìœ¼ë¡œ CacheList ì²˜ë¦¬

### 2. scheduler.py â€” `can_per_seq_trim()` guard
```python
# _can_spec_decode() ë‚´ë¶€
batch = self.batch_generator.active_batch
if batch is not None and not can_per_seq_trim(batch.cache):
    return False
```
- cacheê°€ `trim_per_sequence`ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ spec decode ë¹„í™œì„±í™”

### 3. scheduler.py â€” noop_trim off-by-one fix
```python
# ë³€ê²½ ì „
noop_trim = spec_pending_state["max_draft_len"]

# ë³€ê²½ í›„
noop_trim = spec_pending_state["max_draft_len"] + 1
```
- forwardê°€ `y + drafts`ë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ trim amountëŠ” `k+1`ì´ì–´ì•¼ í•¨

### 4. scheduler.py â€” mx scope fix
- `_step_spec_decode_tp()` ë‚´ë¶€ì— `import mlx.core as mx`ê°€ ìˆì–´ top-level importë¥¼ shadow â†’ ì œê±°

---

## ìˆ˜ì •ëœ ì½”ë“œ ëª©ë¡

| íŒŒì¼ | ë³€ê²½ | ì»¤ë°‹ |
|------|------|------|
| `distributed_launcher.py` | RuntimeError guard, worker batch state sync, ready barrier | `0428e89`, `f258b74` |
| `scheduler.py` | emission ë¡œì§, can_per_seq_trim guard, noop_trim +1, mx scope fix | `0428e89`, `ad2d1dc` |
| `engine_core.py` | memory threshold 500GiB, cache materialization | `0428e89`, `4b5a16f` |
| `spec_decode/cache_utils.py` | `_trim_layer()` ì¬ê·€, `batch_variable_trim` ìˆ˜ì • | `0428e89` |
| `scheduler.py`, `distributed_launcher.py` | _synced_step ëª½í‚¤íŒ¨ì¹˜ (ë¶„ì‚° ìƒ˜í”Œë§ ë™ê¸°í™”) | `d11cd16` |

---

## ë²„ê·¸ 2 ì¬ë°œê²¬: Output Corruption â€” State Accounting Desync (OPEN)

> ë°œê²¬ì¼: 2026-02-16
> ìƒíƒœ: **ë¯¸ìˆ˜ì • â€” ì‹¤ì œ í…ŒìŠ¤íŠ¸ì—ì„œ ì¬í˜„ë¨**

### ì´ì „ ìˆ˜ì •ì˜ í•œê³„

ì»¤ë°‹ `0428e89`ì—ì„œ emission ë¡œì§ì„ `[batch_y] + accepted[:-1]`ë¡œ ìˆ˜ì •í–ˆìœ¼ë‚˜, ì´ëŠ” **emission ê²½ë¡œë§Œ** ìˆ˜ì •í•œ ê²ƒ. batch state ì—…ë°ì´íŠ¸ ê²½ë¡œëŠ” ì—¬ì „íˆ raw `accepted_tokens`ë¥¼ ì‚¬ìš©í•˜ì—¬ drift ë°œìƒ.

### ì¬í˜„ ê²°ê³¼ (2026-02-16)

```
í™˜ê²½: Kimi K2.5, TP=2, n-gram k=3, temperature=0.0
64 í† í°: 10.5 tok/s (ê²½ë¯¸í•œ ì´ìƒ)
256 í† í°: 16.0 tok/s (ì‹¬ê°í•œ corruption)

ì¶œë ¥ ì˜ˆì‹œ:
- "a a technical content" (ë‹¨ì–´ ì¤‘ë³µ)
- "detailed detailed and detailed" (ë°˜ë³µ)
- "22. Why it's needed" (ë²ˆí˜¸ ê¹¨ì§)
- ëë¶€ë¶„: "é€ é€ é€ é€ é€ é€ ..." (ì™„ì „ ë¶•ê´´)
```

### ê·¼ë³¸ ì›ì¸ (Codex ë¶„ì„)

4ê°€ì§€ ê²½ë¡œê°€ ê°ê° ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ committed tokensë¥¼ ê³„ì‚°:

| ê²½ë¡œ | ë°ì´í„° ì†ŒìŠ¤ | Clipping ì ìš© | ìƒíƒœ |
|------|-----------|-------------|------|
| Emission (ì‘ë‹µ ì „ì†¡) | `[batch_y] + accepted[:-1]` | âœ… stop/length ì ìš© | ì •í™• |
| `batch.tokens` ì—…ë°ì´íŠ¸ | `[batch_y] + accepted[:-1]` | âŒ ë¯¸ì ìš© | drift |
| `batch.num_tokens` ì—…ë°ì´íŠ¸ | `len(accepted_tokens)` (bonus í¬í•¨) | âŒ ë¯¸ì ìš© | drift |
| Worker ë¯¸ëŸ¬ (SpecDecodeResult) | raw `accepted_tokens` ê¸°ë°˜ | âŒ ë¯¸ì ìš© | drift |

ì´ driftê°€ ë§¤ spec decode stepë§ˆë‹¤ ëˆ„ì  â†’ n-gram proposer historyì™€ KV cache ìƒíƒœ ì–´ê¸‹ë‚¨ â†’ í† í° ì¤‘ë³µ/ë°˜ë³µ â†’ ì¥ê¸° ìš”ì²­ì—ì„œ ì™„ì „ ë¶•ê´´.

### ìˆ˜ì • í”Œëœ

#### Phase 1: Canonical Committed List í†µí•©

`_step_spec_decode_tp()` (scheduler.py) ìˆ˜ì •:

1. **Emission ë£¨í”„ì—ì„œ `canonical_committed` ë¦¬ìŠ¤íŠ¸ ìƒì„±**
   - stop/length clipping í›„ **ì‹¤ì œ emitëœ í† í°ë§Œ** í¬í•¨
   - rollbackëœ í† í°ì€ ì œì™¸

2. **batch.tokens ì—…ë°ì´íŠ¸ë¥¼ canonical_committed ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½**
   ```python
   # ë³€ê²½ ì „ (í˜„ì¬)
   tokens_for_cache = result.accepted_tokens[:-1] if result.accepted_tokens else []
   new_tokens = [batch_y[batch_idx]] + tokens_for_cache
   n_committed = len(result.accepted_tokens)

   # ë³€ê²½ í›„
   new_tokens = canonical_committed[rid]  # emissionì—ì„œ ê³„ì‚°ëœ ê²ƒê³¼ ë™ì¼
   n_committed = len(canonical_committed[rid])
   ```

3. **batch.num_tokensë„ canonical ê¸°ë°˜ìœ¼ë¡œ**
   ```python
   batch.num_tokens[batch_idx] += len(canonical_committed[rid])
   ```

#### Phase 2: SpecDecodeResultì— canonical ì •ë³´ í¬í•¨

Workerì— broadcastí•˜ëŠ” `SpecDecodeResult`ì— canonical committed ì •ë³´ ì¶”ê°€:

```python
spec_result = SpecDecodeResult(
    step_id=self._step_count,
    accepted_tokens=canonical_committed,  # raw â†’ canonicalë¡œ ë³€ê²½
    trim_amounts=trim_amounts,
    new_y=new_y,
    finished_ids=finished_in_spec,
)
```

#### Phase 3: Worker ë¯¸ëŸ¬ ë™ê¸°í™”

`distributed_launcher.py`ì˜ worker spec decode ì²˜ë¦¬ì—ì„œë„ canonical ê¸°ë°˜ ì—…ë°ì´íŠ¸:
- `batch.tokens[batch_idx]` = canonical committed í† í° ì¶”ê°€
- `batch.num_tokens[batch_idx]` += canonical committed ìˆ˜

#### Phase 4: ê²€ì¦

1. 64 í† í° ìš”ì²­ â€” corruption ì—†ëŠ” coherent ì¶œë ¥
2. 256 í† í° ìš”ì²­ â€” corruption ì—†ì´ ì™„ë£Œ
3. 500+ í† í° ìš”ì²­ â€” ì¥ê¸° ì•ˆì •ì„± í™•ì¸
4. Acceptance rate > 0% í™•ì¸ (n-gram íŒ¨í„´ ë§¤ì¹­ ì‹œ)

### ê´€ë ¨ ì½”ë“œ ìœ„ì¹˜

| íŒŒì¼ | ë¼ì¸ | ì„¤ëª… |
|------|------|------|
| `scheduler.py` | 978-1003 | Emission ë£¨í”„ (canonical ì†ŒìŠ¤) |
| `scheduler.py` | 1074-1087 | batch.tokens/num_tokens ì—…ë°ì´íŠ¸ (ìˆ˜ì • ëŒ€ìƒ) |
| `scheduler.py` | 1102-1112 | SpecDecodeResult broadcast (ìˆ˜ì • ëŒ€ìƒ) |
| `distributed_launcher.py` | 424-450 | Worker batch state ë¯¸ëŸ¬ (ìˆ˜ì • ëŒ€ìƒ) |

---

## ë²„ê·¸ 5: TP ì¶œë ¥ Corruption ê·¼ë³¸ ì›ì¸ â€” ìƒ˜í”Œë§ ë™ê¸°í™” ëˆ„ë½ â€” âœ… ìˆ˜ì • ì™„ë£Œ + ê²€ì¦ ì™„ë£Œ

> ë°œê²¬ì¼: 2026-02-16
> ìƒíƒœ: **ìˆ˜ì • ì™„ë£Œ + ê²€ì¦ ì™„ë£Œ** (ì»¤ë°‹ `d11cd16`)

### ë°œê²¬ ê²½ìœ„

ë²„ê·¸ 2ì˜ state accounting desync ìˆ˜ì • í›„ì—ë„ corruptionì´ ì¬í˜„ë¨. Baseline ë¹„êµ í…ŒìŠ¤íŠ¸ (spec decode ì—†ì´ normal decode)ì—ì„œë„ **ë™ì¼í•œ corruption íŒ¨í„´** í™•ì¸ â†’ spec decodeê°€ ì•„ë‹Œ TP ìì²´ì˜ ë¬¸ì œ.

### ê·¼ë³¸ ì›ì¸

mlx-lmì˜ `BatchGenerator._step()` ë‚´ë¶€ì— ë¶„ì‚° ìƒ˜í”Œë§ ë™ê¸°í™”ê°€ ë‚´ì¥ë˜ì–´ ìˆìŒ:

```python
# mlx-lm generate.py _step() ë‚´ë¶€
if self._dist_group is not None:
    if self._dist_rank > 0:
        sampled = mx.zeros_like(sampled)
    sampled = mx.distributed.all_sum(sampled, group=self._dist_group)
```

**ê·¸ëŸ¬ë‚˜** `BatchGenerator` ìƒì„± ì‹œ `dist_group`ì„ ì „ë‹¬í•˜ì§€ ì•Šì•„ì„œ ì´ ë™ê¸°í™”ê°€ **ì–‘ìª½ Rankì—ì„œ ëª¨ë‘ ë¹„í™œì„±**:

- Rank 0 (scheduler.py:1429): `BatchGenerator(model=..., ...)` â€” dist_group ëˆ„ë½
- Worker (distributed_launcher.py:513): `BatchGenerator(model=..., ...)` â€” dist_group ëˆ„ë½

### ê²°ê³¼

1. ì–‘ Rankê°€ ë…ë¦½ì ìœ¼ë¡œ ìƒ˜í”Œë§ â†’ ë‹¤ë¥¸ í† í° ìƒì„±
2. ë‹¤ë¥¸ í† í°ìœ¼ë¡œ model forward í˜¸ì¶œ â†’ all_sumì´ ë¶ˆì¼ì¹˜ ë°ì´í„° í•©ì‚°
3. KV cacheì— ì˜ëª»ëœ K/V ê°’ ì˜êµ¬ ì €ì¥
4. ì´í›„ attentionì´ ì˜¤ì—¼ëœ KV cache ì°¸ì¡° â†’ ì ì§„ì  í‡´í™”
5. ì´ˆê¸° í† í°ì€ í™•ë¥ ì´ ë†’ì•„ ìš°ì—°íˆ ì¼ì¹˜í•˜ë¯€ë¡œ ì •ìƒìœ¼ë¡œ ë³´ì´ë‹¤ê°€, ê¸¸ì–´ì§ˆìˆ˜ë¡ ë°œì‚°

### ì™œ 64í† í°ì€ ê´œì°®ê³  256í† í°ì—ì„œ ê¹¨ì§€ëŠ”ê°€

- ì´ˆê¸° í† í°ë“¤: ë§¤ìš° ë†’ì€ í™•ë¥  (>90%)ë¡œ ì–‘ Rankê°€ ê°™ì€ í† í° ìƒ˜í”Œë§ â†’ ì™¸ê²¬ìƒ ì •ìƒ
- 50~100í† í° ì´í›„: í™•ë¥  ë¶„í¬ê°€ í‰í‰í•´ì§€ë©° Rank ê°„ í† í° ë°œì‚° ì‹œì‘
- ë°œì‚° ì‹œ KV cache ì˜¤ì—¼ â†’ ì´í›„ ëª¨ë“  attention ì˜í–¥ â†’ ë³µë¦¬ì  í‡´í™”
- 256í† í°: "é€ é€ é€ é€ ..." ì™„ì „ ë¶•ê´´ ë„ë‹¬

### ìˆ˜ì •: _synced_step ëª½í‚¤íŒ¨ì¹˜ (ì»¤ë°‹ `d11cd16`)

BatchGenerator ë‚´ë¶€ `_step()` ë©”ì„œë“œë¥¼ ê°ì‹¸ì„œ ë¶„ì‚° ìƒ˜í”Œë§ ë™ê¸°í™”ë¥¼ ì£¼ì…í•˜ëŠ” ë°©ì‹:

#### scheduler.py â€” Rank 0 BatchGenerator ëª½í‚¤íŒ¨ì¹˜

```python
# BatchGenerator ìƒì„± í›„
if self._communicator is not None and self._communicator.is_distributed:
    _orig_step = bg._step
    _comm = self._communicator

    def _synced_step(input_tokens, prompt_cache, samplers, logits_processors, tokens):
        sampled, logprobs = _orig_step(
            input_tokens, prompt_cache, samplers, logits_processors, tokens
        )
        if _comm.rank > 0:
            sampled = mx.zeros_like(sampled)
        sampled = mx.distributed.all_sum(sampled, group=_comm.group)
        return sampled, logprobs

    bg._step = _synced_step
```

#### distributed_launcher.py â€” Worker BatchGenerator ëª½í‚¤íŒ¨ì¹˜

ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ workerì˜ BatchGeneratorì—ë„ ì ìš©:

```python
if communicator.is_distributed:
    _orig_step = batch_generator._step

    def _synced_step(input_tokens, prompt_cache, samplers, logits_processors, tokens):
        sampled, logprobs = _orig_step(
            input_tokens, prompt_cache, samplers, logits_processors, tokens
        )
        if communicator.rank > 0:
            sampled = mx.zeros_like(sampled)
        sampled = mx.distributed.all_sum(sampled, group=communicator.group)
        return sampled, logprobs

    batch_generator._step = _synced_step
```

**ì›ë¦¬**: Rank 0ë§Œ ì‹¤ì œ ìƒ˜í”Œ ê°’ì„ ê¸°ì—¬í•˜ê³ , ë‹¤ë¥¸ Rankì€ zerosë¥¼ ê¸°ì—¬. `all_sum` í›„ ëª¨ë“  Rankì— ë™ì¼í•œ í† í° IDê°€ ì „íŒŒë¨.

**dist_group ì§ì ‘ ì „ë‹¬ ëŒ€ì‹  ëª½í‚¤íŒ¨ì¹˜ë¥¼ ì„ íƒí•œ ì´ìœ **: mlx-lmì˜ `BatchGenerator`ì— `dist_group`ì„ ì „ë‹¬í•˜ë©´ ë‚´ë¶€ ë™ê¸°í™”ê°€ í™œì„±í™”ë˜ì§€ë§Œ, ì´ëŠ” normal decode ê²½ë¡œì—ì„œë§Œ ë™ì‘í•˜ê³  spec decode verify ê²½ë¡œì—ì„œëŠ” ë³„ë„ì˜ ë™ê¸°í™”ê°€ í•„ìš”. ëª½í‚¤íŒ¨ì¹˜ ë°©ì‹ì€ ëª¨ë“  ê²½ë¡œì—ì„œ íˆ¬ëª…í•˜ê²Œ ë™ì‘.

### ì§„ë‹¨ ê²€ì¦ ê²°ê³¼ (2026-02-16)

ì„ì‹œ ì§„ë‹¨ ì½”ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ì–‘ Rankì˜ ìƒ˜í”Œë§ ê²°ê³¼ë¥¼ ë¹„êµ:

**ì§„ë‹¨ ë°©ë²•**: `_synced_step` ë‚´ë¶€ì— ì¶”ê°€ `all_sum`ì„ ì‚½ì…. ì–‘ Rankê°€ raw ìƒ˜í”Œ ê°’ì„ ê¸°ì—¬í•˜ì—¬ í•©ì‚° â†’ `token_sum == 2 * local_token`ì´ë©´ MATCH, ì•„ë‹ˆë©´ MISMATCH.

**ê²°ê³¼ (temp=0, 256 tokens)**:
```
Rank 0: 116 MATCH, 0 MISMATCH
Rank 1: 116 MATCH, 0 MISMATCH
```

- **0 MISMATCH**: 256 ìŠ¤í… ì „ì²´ì—ì„œ ì–‘ Rankê°€ ì •í™•íˆ ë™ì¼í•œ í† í°ì„ ìƒ˜í”Œë§
- ì´ëŠ” _synced_step ëª½í‚¤íŒ¨ì¹˜ê°€ ì •ìƒ ë™ì‘í•¨ì„ ì¦ëª…
- **ê·¸ëŸ¬ë‚˜** ì¶œë ¥ í’ˆì§ˆ ìì²´ëŠ” ì—¬ì „íˆ ì €í•˜ë¨ â†’ ë²„ê·¸ 6 ì°¸ì¡°

### ê²€ì¦ ê²°ê³¼

1. âœ… 64 í† í° ìš”ì²­ â€” ê²½ë¯¸í•œ ì´ìƒë§Œ ê´€ì°° (ê°œì„ ë¨)
2. âœ… 256 í† í° ìš”ì²­ â€” ì–‘ Rank ë™ì¼ í† í° í™•ì¸ (0 MISMATCH)
3. âš ï¸ Baseline (spec decode ì—†ìŒ) â€” corruption ì—¬ì „íˆ ì¡´ì¬ â†’ ë²„ê·¸ 6
4. âŒ 500+ í† í° â€” ë¯¸í…ŒìŠ¤íŠ¸

### ê´€ë ¨ ì½”ë“œ ìœ„ì¹˜

| íŒŒì¼ | ë¼ì¸ | ì„¤ëª… |
|------|------|------|
| `scheduler.py` | 1440-1458 | _synced_step ëª½í‚¤íŒ¨ì¹˜ (ìˆ˜ì •ë¨) |
| `distributed_launcher.py` | 519-536 | Worker _synced_step ëª½í‚¤íŒ¨ì¹˜ (ìˆ˜ì •ë¨) |
| `distributed_launcher.py` | 639-658 | Normal path cache fixup (ìˆ˜ì •ë¨) |

---

## ë²„ê·¸ 6: TP ëª¨ë“œ ì¶œë ¥ í’ˆì§ˆ ì €í•˜ â€” OPEN

> ë°œê²¬ì¼: 2026-02-16
> ìƒíƒœ: **ë¯¸í•´ê²° â€” ì›ì¸ ì¡°ì‚¬ ì¤‘**

### ì¦ìƒ

- ì–‘ Rankê°€ **ì •í™•íˆ ë™ì¼í•œ í† í°**ì„ ìƒì„± (ë²„ê·¸ 5 ìˆ˜ì •ìœ¼ë¡œ í™•ì¸)
- ê·¸ëŸ¬ë‚˜ ì¶œë ¥ ìì²´ì˜ í’ˆì§ˆì´ ì €í•˜ë¨
- temp=0ì—ì„œë„ 256 í† í° ìˆ˜ì¤€ì—ì„œ ë°˜ë³µ/ë¹„ë¬¸/ë¶•ê´´ ë°œìƒ
- ë‹¨ì¼ ë…¸ë“œ í…ŒìŠ¤íŠ¸ ë¶ˆê°€ (Kimi K2.5 612GB â†’ ë‹¨ì¼ 512GB Mac Studioì— ì ì¬ ë¶ˆê°€)

### ì§„ë‹¨ ê²°ê³¼

- **Inter-rank divergence**: âŒ ì•„ë‹˜ (0 MISMATCH í™•ì¸)
- **MoE routing divergence**: âŒ ì•„ë‹˜ (ê²Œì´íŠ¸ ë¼ìš°íŒ…ì€ ì–‘ Rank ë™ì¼, expertëŠ” hidden dimensionìœ¼ë¡œ ìƒ¤ë”©)
- **Sampling desync**: âŒ ì•„ë‹˜ (_synced_stepìœ¼ë¡œ í•´ê²°)
- **TP êµ¬í˜„ ì •í™•ì„±**: âœ… ê²€ì¦ë¨ (ShardedToAllLinear, MLA, MoE all_sum íŒ¨í„´ ëª¨ë‘ ì •ìƒ)

### ê°€ëŠ¥í•œ ì›ì¸

1. **bfloat16 ì •ë°€ë„ ëˆ„ì **: all_sumì€ bfloat16ìœ¼ë¡œ ìˆ˜í–‰ â†’ 61 ë ˆì´ì–´ Ã— N ìŠ¤í…ì—ì„œ ë°˜ì˜¬ë¦¼ ì˜¤ì°¨ ëˆ„ì 
2. **MLA (Multi-head Latent Attention) TP ìƒí˜¸ì‘ìš©**: ì••ì¶•ëœ KV latent + k_pe ë¶„í• ì´ ì •ë°€ë„ ë¯¼ê°í•  ìˆ˜ ìˆìŒ
3. **int4 ì–‘ìí™” + TP ì¡°í•©**: Kimi K2.5ëŠ” í•™ìŠµ ì‹œë¶€í„° int4 ê¸°ë³¸ì´ë¯€ë¡œ ì–‘ìí™” ìì²´ëŠ” ë¬¸ì œ ì•„ë‹ˆì§€ë§Œ, TP ë¶„í•  + ì–‘ìí™”ì˜ ì¡°í•©ì´ ì˜í–¥ì¤„ ìˆ˜ ìˆìŒ
4. **Generation íŒŒì´í”„ë¼ì¸ ì¼ë°˜ ë²„ê·¸**: TP ë¬´ê´€í•œ ë¬¸ì œì¼ ìˆ˜ ìˆìœ¼ë‚˜, ë‹¨ì¼ ë…¸ë“œ í…ŒìŠ¤íŠ¸ ë¶ˆê°€ë¡œ ê²€ì¦ ë¶ˆê°€

### ë‹¤ìŒ ì¡°ì‚¬ ë°©í–¥

- [ ] float32 all_sum ì‹¤í—˜ (ì •ë°€ë„ ëˆ„ì  ê²€ì¦)
- [ ] ë” ì‘ì€ TP í˜¸í™˜ ëª¨ë¸ë¡œ ë‹¨ì¼ ë…¸ë“œ vs TP í’ˆì§ˆ ë¹„êµ
- [ ] Attention score ë¶„í¬ ë¹„êµ (TP vs ë‹¨ì¼)
- [ ] DeepSeek V3 MLAì˜ kv_latent ë¶„í•  ì •ë°€ë„ ë¶„ì„

---

## ë²„ê·¸ 7: kâ‰¥2 Spec Decode TP ë°ë“œë½ â€” Step 50ì—ì„œ ê²°ì •ë¡ ì  í–‰

> ë°œê²¬ì¼: 2026-02-16
> ìƒíƒœ: **ğŸ”´ í™œë°œíˆ ì¡°ì‚¬ ì¤‘ â€” cache_idx=242ì—ì„œ ê²°ì •ë¡ ì  all_sum ë°ë“œë½ (`878fc00`, `49de3e9`, `4b1c446`)**
> í™˜ê²½: Kimi K2.5, TP=2 (2x Mac Studio M4 Ultra, TB5 RDMA), n-gram speculative decoding

### ì¦ìƒ

- n-gram spec decode k=1ì€ ì •ìƒ ë™ì‘ (20.1 tok/s, acceptance 78.8%)
- kâ‰¥2 (k=2 í…ŒìŠ¤íŠ¸)ì—ì„œ **ì •í™•íˆ step 50ì—ì„œ ë°ë“œë½** ë°œìƒ
- ì¬í˜„ìœ¨ 100% (deterministic)
- ì•½ 110ê°œ í† í° ìƒì„± í›„ í–‰ (50 steps Ã— mean_accepted 1.20 + bonus tokens)
- Rank 0 ë§ˆì§€ë§‰ ë¡œê·¸: `[SpecDecode] steps=50, alpha=0.857, mean_accepted=1.20/2, per_pos=['0.86', '0.85']`
- Rank 1 ë§ˆì§€ë§‰ ë¡œê·¸: startup ì‹œì˜ `Polyfilled BatchKVCache.trim_per_sequence for spec decode` (step-level ë¡œê·¸ ì—†ìŒ)
- ê¸°ì¡´ cb-baseline (spec decode ì—†ìŒ)ë„ ì •ìƒ (15.8-16.2 tok/s)

### ë°°ì œëœ ì›ì¸

1. **Auto-disable**: acceptance rate 0.60 (= 1.20/2) > threshold 0.40 â†’ should_auto_disable() returns False. ë˜í•œ commit 878fc00ì—ì„œ TP ëª¨ë“œì—ì„œ auto-disableì„ ëª…ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”í–ˆìœ¼ë‚˜ ë™ì¼ í–‰ ì¬í˜„
2. **MoE gating routing divergence**: ê²Œì´íŠ¸ ë¼ìš°íŒ…ì€ all_sumëœ ì…ë ¥ì—ì„œ ì—°ì‚° â†’ ì–‘ Rank ë™ì¼
3. **ìƒ˜í”Œë§ desync**: _synced_step ëª½í‚¤íŒ¨ì¹˜ë¡œ ì´ë¯¸ í•´ê²°ë¨ (ë²„ê·¸ 5)

### í˜„ì¬ ê°€ì„¤

1. **ëˆ„ì  batch state drift**: k=2ì—ì„œëŠ” ë§¤ step ê°€ë³€ rollback (0~2 í† í°) â†’ 50 steps ë™ì•ˆ cache _idx, offset, left_padding ë“±ì´ Rank ê°„ ë¯¸ì„¸ ì°¨ì´ ëˆ„ì  â†’ protocol mismatchë¡œ ë°ë“œë½
2. **cache _idx desync**: batch_variable_trim í›„ cache _idx ì¬ê³„ì‚°ì´ Rank 0ê³¼ workerì—ì„œ ë‹¤ë¥´ê²Œ ì§„í–‰ë  ê°€ëŠ¥ì„±
3. **3-token input íŠ¹ìˆ˜ ì¼€ì´ìŠ¤**: k=2ì—ì„œëŠ” [y, d1, d2] 3í† í° ì…ë ¥ â†’ k=1ì˜ [y, d1] 2í† í°ê³¼ ë‹¤ë¥¸ edge case ì¡´ì¬ ê°€ëŠ¥
4. **Step 50 íŠ¹ìˆ˜ì„±**: rolling window (maxlen=50)ì´ ê°€ë“ ì°¨ëŠ” ì‹œì  â€” auto-disableì€ ë°œë™ ì•ˆ í•˜ì§€ë§Œ, metrics ê´€ë ¨ ë‹¤ë¥¸ side effect ê°€ëŠ¥

### ì ìš©ëœ ìˆ˜ì • (commit 878fc00, íš¨ê³¼ ì—†ìŒ)

1. TP ëª¨ë“œì—ì„œ auto-disable ë¹„í™œì„±í™” (scheduler.py)
2. cache _idx consistency check + fixup_cache_after_filter ì¶”ê°€ (scheduler.py)
3. batch_variable_trim í›„ fixup_cache_after_filter í˜¸ì¶œ (scheduler.py)
4. DEBUG ë ˆë²¨ ì§„ë‹¨ ë¡œê¹… (ì„œë²„ INFO ë ˆë²¨ì´ë¼ ì¶œë ¥ ì•ˆ ë¨)
5. INFO ë ˆë²¨ ì§„ë‹¨ ë¡œê¹… (`49de3e9`) â†’ ë°ë“œë½ ìœ„ì¹˜ íŠ¹ì • (model forward ë‚´ë¶€)
6. mx.eval() ë°°ë¦¬ì–´ 5ê³³ ì¶”ê°€ (`4b1c446`) â†’ íš¨ê³¼ ì—†ìŒ, ë™ì¼ ìœ„ì¹˜ì—ì„œ í–‰

### ì§„ë‹¨ ê²°ê³¼ (2026-02-16, commit `49de3e9` + `4b1c446`)

INFO ë ˆë²¨ ì§„ë‹¨ ë¡œê¹… (`[SD-TP]`, `[SD-W]` prefix)ì„ ë°°í¬í•˜ì—¬ ë°ë“œë½ ìœ„ì¹˜ë¥¼ ì •í™•íˆ íŠ¹ì •:

**ë‘ ë²ˆì˜ ì¬í˜„ì—ì„œ ë™ì¼í•œ ê²°ê³¼:**
- Rank 0: `step=N PRE-FORWARD input_shape=(1, 2) cache_idx=242` â† ë§ˆì§€ë§‰ ë¡œê·¸
- Rank 1: `PRE-FORWARD input_shape=(1, 2) cache_idx=242` â† ë§ˆì§€ë§‰ ë¡œê·¸
- **ì–‘ìª½ Rank ëª¨ë‘ ë™ì¼í•œ ìƒíƒœë¡œ model forwardì— ì§„ì… í›„ all_sum ë‚´ë¶€ì—ì„œ ë°ë“œë½**
- í”„ë¡œí† ì½œ desync ì•„ë‹˜ (cache_idx, input_shape ëª¨ë‘ ì¼ì¹˜)

**ì‹œë„í•œ ìˆ˜ì •ê³¼ ê²°ê³¼:**
1. `fixup_cache_after_filter` ì¶”ê°€ (`878fc00`) â†’ í–‰ì´ step 50ì—ì„œ step ~87ë¡œ ì´ë™ (ì§€ì—°ë§Œ, í•´ê²° ì•„ë‹˜)
2. `mx.eval()` ë°°ë¦¬ì–´ ì¶”ê°€ (`4b1c446`) â†’ **íš¨ê³¼ ì—†ìŒ**, ë™ì¼í•œ cache_idx=242ì—ì„œ í–‰

**ë°°ì œëœ ì¶”ê°€ ê°€ì„¤:**
- **Lazy evaluation ëˆ„ì **: mx.eval ë°°ë¦¬ì–´ë¥¼ 5ê³³ì— ì¶”ê°€í–ˆìœ¼ë‚˜ ë™ì¼ ìœ„ì¹˜ì—ì„œ í–‰ â†’ lazy graph ì•„ë‹˜
- **í”„ë¡œí† ì½œ desync**: ì–‘ Rankì˜ cache_idx, input_shape ì™„ì „ ì¼ì¹˜ â†’ í†µì‹  í”„ë¡œí† ì½œ ë¬¸ì œ ì•„ë‹˜

**ë‚¨ì€ ê°€ì„¤:**
1. **cache_idx=242 íŠ¹ìˆ˜ì„±**: ë‘ ë²ˆì˜ ì‹¤í–‰ì—ì„œ ì •í™•íˆ ê°™ì€ cache í¬ê¸°ì—ì„œ í–‰ â†’ KV cache ìš©ëŸ‰ í•œê³„ ë˜ëŠ” Metal ë²„í¼ í¬ê¸° ì œí•œ ê°€ëŠ¥ì„±
2. **JACCL/RDMA ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ**: ëˆ„ì ëœ all_sum ì—°ì‚° í›„ RDMA ë²„í¼ ë¶€ì¡±
3. **spec decode ê²½ë¡œ ê³ ìœ ì˜ cache ë ˆì´ì•„ì›ƒ ë¬¸ì œ**: baselineì€ 256+ í† í° ì •ìƒ ì™„ë£Œ (cache_idx > 273), spec decodeë§Œ 242ì—ì„œ í–‰

### ë‹¤ìŒ ì¡°ì‚¬ ë‹¨ê³„

1. spec decode cache í• ë‹¹ í¬ê¸° í™•ì¸ (max_tokens + k ê³ ë ¤ ì—¬ë¶€)
2. ë” ì‘ì€ max_tokens (ì˜ˆ: 128)ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ cache_idx í•œê³„ì  ê²€ì¦
3. all_sum í˜¸ì¶œ íšŸìˆ˜ ëŒ€ë¹„ baselineê³¼ ë¹„êµ
4. model forward ë‚´ë¶€ ë ˆì´ì–´ë³„ ë¡œê¹… ì¶”ê°€

### ê´€ë ¨ ì½”ë“œ ìœ„ì¹˜

| íŒŒì¼ | ë¼ì¸ | ì„¤ëª… |
|------|------|------|
| `scheduler.py` | 723-734 | TP auto-disable ë¹„í™œì„±í™” (878fc00) |
| `scheduler.py` | 959-970 | cache _idx consistency check (878fc00) |
| `scheduler.py` | 1082-1084 | fixup_cache_after_filter after trim (878fc00) |
| `scheduler.py` | 970-992 | INFO ì§„ë‹¨ ë¡œê¹… (ë¯¸ë°°í¬) |
| `distributed_launcher.py` | 400-420 | Worker INFO ì§„ë‹¨ ë¡œê¹… (ë¯¸ë°°í¬) |

---

## ì¸í”„ë¼ ì´ìŠˆ (ìš´ì˜ ì°¸ê³ )

### Metal Wired Memory Leak
- `kill -9` í›„ Metal GPU memory ë¯¸í•´ì œ â†’ ë¦¬ë¶€íŠ¸ë§Œ í•´ê²°
- ì •ìƒ wired: ~5GB (340k pages), ëˆ„ìˆ˜ ì‹œ: 300~356GB (20M+ pages)
- **ë°˜ë“œì‹œ SIGTERM ë¨¼ì €, ì•ˆ ì£½ìœ¼ë©´ SIGKILL**

### nohup + exec ì·¨ì•½ì 
- `start_server_rank.sh`: âœ… ìˆ˜ì • ì™„ë£Œ
- `start_server_rank_specngram.sh`: âš ï¸ ë¯¸ìˆ˜ì • (ì—¬ì „íˆ exec ì‚¬ìš©)

### JACCL EBUSY
- kill í›„ 30ì´ˆ ëŒ€ê¸° í•„ìˆ˜ (RDMA ìì› í•´ì œ ëŒ€ê¸°)
